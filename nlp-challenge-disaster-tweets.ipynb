{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\nYou can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom tqdm.notebook import tqdm_notebook\nimport re\n\ntqdm_notebook.pandas()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:12.102954Z","iopub.execute_input":"2023-06-01T19:34:12.103359Z","iopub.status.idle":"2023-06-01T19:34:12.232263Z","shell.execute_reply.started":"2023-06-01T19:34:12.103327Z","shell.execute_reply":"2023-06-01T19:34:12.231330Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:12.233961Z","iopub.execute_input":"2023-06-01T19:34:12.234452Z","iopub.status.idle":"2023-06-01T19:34:12.279181Z","shell.execute_reply.started":"2023-06-01T19:34:12.234424Z","shell.execute_reply":"2023-06-01T19:34:12.278219Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/glove-twitter-27b-fast/GloVe-Twitter-27B_Fast/glove.twitter.27B.100d.txt\n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/preprocessed-tweets/preprocessed.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/lorenzo-mora/NLP-Challenge-Disaster-Tweets.git\n%cd /kaggle/working/NLP-Challenge-Disaster-Tweets\n!git pull","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:12.280418Z","iopub.execute_input":"2023-06-01T19:34:12.280790Z","iopub.status.idle":"2023-06-01T19:34:13.715474Z","shell.execute_reply.started":"2023-06-01T19:34:12.280748Z","shell.execute_reply":"2023-06-01T19:34:13.714115Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sys.path.insert(1, '/kaggle/working/NLP-Challenge-Disaster-Tweets')","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:13.718124Z","iopub.execute_input":"2023-06-01T19:34:13.718511Z","iopub.status.idle":"2023-06-01T19:34:13.725964Z","shell.execute_reply.started":"2023-06-01T19:34:13.718477Z","shell.execute_reply":"2023-06-01T19:34:13.724592Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/preprocessed-tweets/preprocessed.csv\", header = 0)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:07:48.642982Z","iopub.execute_input":"2023-06-01T20:07:48.643332Z","iopub.status.idle":"2023-06-01T20:07:48.686632Z","shell.execute_reply.started":"2023-06-01T20:07:48.643295Z","shell.execute_reply":"2023-06-01T20:07:48.684985Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"---\n## Preprocessing","metadata":{}},{"cell_type":"code","source":"import preprocessing\nfrom preprocessing import clean_text, stemming, lemmatization, extract_hashtags, extract_tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", header = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.text[150]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['cleaned_text'] = data.text.progress_apply(lambda t: clean_text(t, False, False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.cleaned_text[150]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rows with empty text after preprocessing\ndata = data.drop(data[data.cleaned_text==''].index).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!python3 -m nltk.downloader wordnet\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['stemmed_text'] = data.cleaned_text.progress_apply(lambda t: stemming(t))\ndata['lemmatized_text'] = data.cleaned_text.progress_apply(lambda t: lemmatization(t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.stemmed_text[150])\nprint(data.lemmatized_text[150])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['hashtags'] = data.text.progress_apply(lambda t: extract_hashtags(t))\ndata['tags'] = data.text.progress_apply(lambda t: extract_tags(t))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data.hashtags[150])\nprint(data.tags[150])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data['tokenized_text'] = data.cleaned_text.progress_apply(lambda t: t.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(data.tokenized_text[150])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_csv('preprocessed.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Data Splitting","metadata":{}},{"cell_type":"code","source":"from classification import DataGenerator as dg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataObj = dg(data.cleaned_text, data.target)\ndataObj.split_data(random_state=42)\ndataObj.tokenize_data(max_sequence_length=20)\ndataObj.data.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Vectorization","metadata":{}},{"cell_type":"code","source":"from word_embedding import GloVe","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:40.283652Z","iopub.execute_input":"2023-06-01T19:34:40.283980Z","iopub.status.idle":"2023-06-01T19:34:49.441356Z","shell.execute_reply.started":"2023-06-01T19:34:40.283958Z","shell.execute_reply":"2023-06-01T19:34:49.440347Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T19:34:49.442981Z","iopub.execute_input":"2023-06-01T19:34:49.443578Z","iopub.status.idle":"2023-06-01T19:34:49.477683Z","shell.execute_reply.started":"2023-06-01T19:34:49.443522Z","shell.execute_reply":"2023-06-01T19:34:49.476470Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   index  id keyword location  \\\n0      0   1     NaN      NaN   \n1      1   4     NaN      NaN   \n2      2   5     NaN      NaN   \n3      3   6     NaN      NaN   \n4      4   7     NaN      NaN   \n\n                                                text  target  \\\n0  Our Deeds are the Reason of this #earthquake M...       1   \n1             Forest fire near La Ronge Sask. Canada       1   \n2  All residents asked to 'shelter in place' are ...       1   \n3  13,000 people receive #wildfires evacuation or...       1   \n4  Just got sent this photo from Ruby #Alaska as ...       1   \n\n                                        cleaned_text  \\\n0       deeds reason earthquake may allah forgive us   \n1              forest fire near la ronge sask canada   \n2  residents asked shelter place notified officer...   \n3  people receive wildfires evacuation orders cal...   \n4  got sent photo ruby alaska smoke wildfires pou...   \n\n                                        stemmed_text  \\\n0          deed reason earthquak mai allah forgiv us   \n1               forest fire near la rong sask canada   \n2  resid ask shelter place notifi offic evacu she...   \n3        peopl receiv wildfir evacu order california   \n4  got sent photo rubi alaska smoke wildfir pour ...   \n\n                                     lemmatized_text                 hashtags  \\\n0         deed reason earthquake may allah forgive u           ['earthquake']   \n1              forest fire near la ronge sask canada                       []   \n2  resident asked shelter place notified officer ...                       []   \n3  people receive wildfire evacuation order calif...            ['wildfires']   \n4  got sent photo ruby alaska smoke wildfire pour...  ['alaska', 'wildfires']   \n\n  tags  \n0   []  \n1   []  \n2   []  \n3   []  \n4   []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n      <th>cleaned_text</th>\n      <th>stemmed_text</th>\n      <th>lemmatized_text</th>\n      <th>hashtags</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n      <td>deeds reason earthquake may allah forgive us</td>\n      <td>deed reason earthquak mai allah forgiv us</td>\n      <td>deed reason earthquake may allah forgive u</td>\n      <td>['earthquake']</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>forest fire near la rong sask canada</td>\n      <td>forest fire near la ronge sask canada</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n      <td>residents asked shelter place notified officer...</td>\n      <td>resid ask shelter place notifi offic evacu she...</td>\n      <td>resident asked shelter place notified officer ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n      <td>people receive wildfires evacuation orders cal...</td>\n      <td>peopl receiv wildfir evacu order california</td>\n      <td>people receive wildfire evacuation order calif...</td>\n      <td>['wildfires']</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n      <td>['alaska', 'wildfires']</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"gl = GloVe('/kaggle/input/glove-twitter-27b-fast/\\\nGloVe-Twitter-27B_Fast/glove.twitter.27B.100d.txt', \"glove_1\")","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:28:22.454387Z","iopub.execute_input":"2023-06-01T20:28:22.454771Z","iopub.status.idle":"2023-06-01T20:28:41.210165Z","shell.execute_reply.started":"2023-06-01T20:28:22.454743Z","shell.execute_reply":"2023-06-01T20:28:41.208793Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"gl.prepare_data(data.cleaned_text, data.target,\n                test_size=0.15, random_state=True, standardize=None)","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:28:45.599768Z","iopub.execute_input":"2023-06-01T20:28:45.600374Z","iopub.status.idle":"2023-06-01T20:28:46.154644Z","shell.execute_reply.started":"2023-06-01T20:28:45.600342Z","shell.execute_reply":"2023-06-01T20:28:46.153427Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"gl.data.keys()","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:28:48.301431Z","iopub.execute_input":"2023-06-01T20:28:48.302441Z","iopub.status.idle":"2023-06-01T20:28:48.310554Z","shell.execute_reply.started":"2023-06-01T20:28:48.302388Z","shell.execute_reply":"2023-06-01T20:28:48.309266Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"dict_keys(['x_train', 'x_test', 'y_train', 'y_test', 'vect_x_train', 'vect_x_test'])"},"metadata":{}}]},{"cell_type":"code","source":"print(gl.data['x_train'][0])\nprint(gl.data['vect_x_train'][0])\nprint(gl.vocabulary[5831], gl.vocabulary[745])","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:28:49.557603Z","iopub.execute_input":"2023-06-01T20:28:49.558018Z","iopub.status.idle":"2023-06-01T20:28:49.567969Z","shell.execute_reply.started":"2023-06-01T20:28:49.557988Z","shell.execute_reply":"2023-06-01T20:28:49.566398Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"deeds reason earthquake may allah forgive us\n[5831, 745, 169, 65, 1603, 5578, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\ndeeds reason\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_vect = gl.data['x_train']\nY_train = gl.data['y_train']\nX_test_vect = gl.data['x_test']\nY_test = gl.data['y_test']","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:28:55.906369Z","iopub.execute_input":"2023-06-01T20:28:55.906768Z","iopub.status.idle":"2023-06-01T20:28:55.913479Z","shell.execute_reply.started":"2023-06-01T20:28:55.906735Z","shell.execute_reply":"2023-06-01T20:28:55.911930Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"gl.embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:29:04.888819Z","iopub.execute_input":"2023-06-01T20:29:04.889189Z","iopub.status.idle":"2023-06-01T20:29:04.895806Z","shell.execute_reply.started":"2023-06-01T20:29:04.889165Z","shell.execute_reply":"2023-06-01T20:29:04.894830Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(14300, 100)"},"metadata":{}}]},{"cell_type":"code","source":"gl.EMBEDDING_DIM","metadata":{"execution":{"iopub.status.busy":"2023-06-01T20:29:05.694548Z","iopub.execute_input":"2023-06-01T20:29:05.694927Z","iopub.status.idle":"2023-06-01T20:29:05.702504Z","shell.execute_reply.started":"2023-06-01T20:29:05.694900Z","shell.execute_reply":"2023-06-01T20:29:05.701048Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}]},{"cell_type":"markdown","source":"---\n# Neural Network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Embedding, Flatten\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\n\nn_test = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if model:\n    n_test += 1\n    del model\n    tf.keras.backend.clear_session()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = Input(shape=(gl.MAX_SEQUENCE_LENGTH, ))\nembeddings = Embedding(input_dim=len(gl.vocabulary)+1,\n                       output_dim=gl.EMBEDDING_DIM,\n                       input_length=gl.MAX_SEQUENCE_LENGTH,\n                       trainable=False,\n                       weights=[gl.embedding_matrix],\n                       name=f\"Embedding_{n_test}1\")\nflatten = Flatten(name=f\"Flatten_{n_test}1\")\ndense_1 = Dense(1024, activation=\"relu\", name=f\"Dense_{n_test}1\")\ndense_2 = Dense(256, activation=\"relu\", name=f\"Dense_{n_test}2\")\ndense_3 = Dense(16, activation=\"relu\", name=f\"Dense_{n_test}3\")\ndense_4 = Dense(1, activation=\"sigmoid\", name=f\"Dense_{n_test}4\")\n\nx = embeddings(inputs)\nx = flatten(x)\nx = dense_1(x)\nx = dense_2(x)\nx = dense_3(x)\noutputs = dense_4(x)\n\nmodel = Model(inputs=inputs, outputs=outputs)\nmodel._name=f\"Test_Model_{n_test}\"\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = Sequential()\n# model.add(Embedding(input_dim=len(gl.vocabulary)+1,\n#                     output_dim=gl.EMBEDDING_DIM,\n#                     input_length=gl.MAX_SEQUENCE_LENGTH,\n#                     trainable=False,\n#                     weights=[gl.embedding_matrix],\n#                     name=f\"Embedding_{n_test}1\"))\n# model.add(Flatten(name=f\"Flatten_{n_test}1\"))\n# model.add(Dense(1024, activation=\"relu\", name=f\"Dense_{n_test}1\"))\n# model.add(Dense(256, activation=\"relu\", name=f\"Dense_{n_test}2\"))\n# model.add(Dense(16, activation=\"relu\", name=f\"Dense_{n_test}3\"))\n# model.add(Dense(1, activation=\"sigmoid\", name=f\"Dense_{n_test}4\")) # softmax\n\n# model._name=f\"Test_Model_{n_test}\"\n# model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=\"adam\",\n              loss=\"squared_hinge\",\n              metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_classes = [\"Neutral\", \"Disaster\"]\nclasses = np.unique(Y_train)\nmapping = dict(zip(classes, target_classes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                      patience=5,\n                                      min_delta=.005,\n                                      restore_best_weights=True,\n                                      start_from_epoch=15)\nmodel.fit(X_train_vect, Y_train,\n          batch_size=128,\n          epochs=300,\n          validation_data=(X_test_vect, Y_test),\n          callbacks=[es])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Y_preds = model.predict(X_test_vect).argmax(axis=-1) # SOFTMAX\nY_preds = model.predict(X_test_vect)\nY_preds = [round(y[0]) for y in Y_preds] # SIGMOID\n\nprint(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_preds)))\nprint(\"\\nClassification Report : \")\nprint(classification_report(Y_test, Y_preds, target_names=target_classes))\nprint(\"\\nConfusion Matrix : \")\nprint(confusion_matrix(Y_test, Y_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skplt.metrics.plot_confusion_matrix([target_classes[i] for i in Y_test],\n                                    [target_classes[i] for i in Y_preds],\n                                    normalize=True,\n                                    title=\"Confusion Matrix\",\n                                    cmap=\"Blues\",\n                                    hide_zeros=True,\n                                    figsize=(5,5)\n                                    )\nplt.xticks(rotation=90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Test","metadata":{}},{"cell_type":"code","source":"# Add escape character in emoticons\nimport emot\nfrom emot.emo_unicode import EMOTICONS_EMO\n\ndef insert_escape(text:str)->str:\n\n    chars = r'[\\+|\\[|\\\\|\\^|\\{|\\(|\\*|\\||\\}|\\.|\\]|\\?|\\$|\\)|\\/]'\n    escape = \"\\\\\"\n\n    res = [i.start() for i in re.finditer(chars, text)]\n\n    for i in range(len(res)):\n        idx = res[i]\n        text = text[:idx+i] + escape + text[idx+i:]\n    return text\n\nnew_emoticons = []\nfor s in EMOTICONS_EMO:\n    temp = insert_escape(s)\n    print(temp)\n    new_emoticons.append(temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Text in Uicode\ntext = \"üòÇ ‚ù§ ‚òÆ üôÇ ‚ù§ ¬©\"\n''.join(r'\\u{:04X}'.format(ord(chr)) for chr in text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}